{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Feature Model\n",
    "\n",
    "Here I use gensim implementation of Latent Dirichlet Allocation (LDA) to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import ldamodel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It receives a 2d array of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary.items(): [(0, 'top'), (1, 'pokemon'), (3, 'pictures'), (2, 'incredible')]\n",
      "\n",
      "corpus: [[(0, 1), (1, 1), (2, 1)], [(2, 1)], [(3, 1)]]\n",
      "\n",
      "LDA Topics:\n",
      "\n",
      "Topic 0\n",
      "0.406*incredible + 0.257*top + 0.255*pokemon + 0.082*pictures\n",
      "\n",
      "Topic 1\n",
      "0.490*pictures + 0.200*incredible + 0.157*pokemon + 0.153*top\n",
      "\n",
      "Topic 2\n",
      "0.303*incredible + 0.266*pictures + 0.217*pokemon + 0.214*top\n"
     ]
    }
   ],
   "source": [
    "def test_model(documents, num_topics):\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    np.random.seed(0)  # Set a fixed seed cuz it's non-deterministic\n",
    "    lda = ldamodel.LdaModel(corpus, id2word=dictionary, \n",
    "                            num_topics=num_topics, iterations=100000,\n",
    "                            alpha='symmetric', gamma_threshold=0.001,\n",
    "                            eta=None)\n",
    "    \n",
    "    print(\"dictionary.items(): %s\\n\" % list(dictionary.items()))\n",
    "    # Converted into a bag of words format - (token_id, token_count) 2-tuples\n",
    "    print(\"corpus: %s\\n\" % corpus)\n",
    "    print(\"LDA Topics:\")\n",
    "    for topic in range(num_topics):\n",
    "        print(\"\\nTopic %s\" % topic)\n",
    "        print(lda.print_topic(topic))\n",
    "    return lda, dictionary\n",
    "\n",
    "test_documents = [[\"top\", \"pokemon\", \"incredible\"], \n",
    "                  [\"incredible\"], \n",
    "                  [\"pictures\"]]\n",
    "\n",
    "num_topics = 3\n",
    "test_lda, test_dict = test_model(test_documents, num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.82723213947096941), (1, 0.085512135625795294), (2, 0.08725572490323534)]\n",
      "Most probable topic: (0, 0.82723213947096941)\n",
      "Most probable word in topic: ('incredible', 0.40624497455174713)\n",
      "\n",
      "[(0, 0.65069628706313964), (1, 0.17177125240199564), (2, 0.17753246053486471)]\n",
      "Most probable topic: (0, 0.65069628706313964)\n",
      "Most probable word in topic: ('incredible', 0.40624497455174713)\n",
      "\n",
      "[(0, 0.16791207285071336), (1, 0.65878304940081323), (2, 0.17330487774847336)]\n",
      "Most probable topic: (1, 0.65878304940081323)\n",
      "Most probable word in topic: ('pictures', 0.48988096504845463)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.82723213947096941, 0.085512135625795294, 0.08725572490323534],\n",
       " [0.65069628706313964, 0.17177125240199564, 0.17753246053486471],\n",
       " [0.16791207285071336, 0.65878304940081323, 0.17330487774847336]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def extract_features(docs):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        np.random.seed(0)  # Set a fixed seed cuz it's non-deterministic\n",
    "        topic_distribution = test_lda[test_dict.doc2bow(doc)]\n",
    "        print(topic_distribution)\n",
    "        if len(topic_distribution) == 0:\n",
    "            print(\"Empty\")\n",
    "        else:\n",
    "            most_probable = max(topic_distribution,key=itemgetter(1))\n",
    "            print(\"Most probable topic: {}\".format(most_probable))\n",
    "            print(\"Most probable word in topic: {}\\n\".format(\n",
    "                    test_lda.show_topic(most_probable[0], 1)[0]\n",
    "                ))\n",
    "\n",
    "            # iterate over topic prediction tuples\n",
    "            values_array = []\n",
    "            for key, value in topic_distribution:\n",
    "                values_array.append(value)\n",
    "            res.append(values_array)\n",
    "    return res\n",
    "\n",
    "extract_features(test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Those probability distributions are commonly used as features**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
